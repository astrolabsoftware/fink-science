{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc43cf7-bbf3-46ac-a2ed-f569131b6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7746c9a3-a3a5-4dfe-b5e9-2918d59b2698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 08:22:01 WARN Utils: Your hostname, andre-PC resolves to a loopback address: 127.0.1.1; using 192.168.100.55 instead (on interface eno1)\n",
      "22/09/07 08:22:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/09/07 08:22:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb80f55a-66c5-4797-b8d1-4260b9283293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 08:22:04 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR='/home/data/phd/research_projects/fink_science/fink_science/cbpf_classifier/data/alerts/'\n",
    "df = (spark\n",
    "         .read\n",
    "         .format('parquet')\n",
    "         .load(DATA_DIR+'part-00000-c1d37f00-5125-44ed-95d3-169d6b7395c3-c000.snappy.parquet')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7be80a-d2e2-4b3e-9a36-bd9930467c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59436559-71b3-44d9-a509-9d1cf7f76275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05049daa-92ff-4537-8586-d5c534240c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(df['diaSource']['diaSourceId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0dece90-ddc2-429c-b3db-74da80e46fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select((df\n",
    "#            .diaSource\n",
    "#            .diaSourceId\n",
    "#           )\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020f309-41e8-4b8b-9754-aeb77670c8a4",
   "metadata": {},
   "source": [
    "### pypsark DataFrame format (only showed features used by classifier)\n",
    "\n",
    "```\n",
    "root\n",
    " |-- alertId: long (nullable = true)\n",
    " |-- diaSource: struct (nullable = true)\n",
    " |    |-- midPointTai: double (nullable = true)\n",
    " |    |-- filterName: string (nullable = true)\n",
    " |    |-- psFlux: float (nullable = true)\n",
    " |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaSources: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |    |    |-- psFlux: float (nullable = true)\n",
    " |    |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaForcedSources: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |    |    |-- psFlux: float (nullable = true)\n",
    " |    |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaNondetectionLimits: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |-- diaObject: struct (nullable = true)\n",
    " |    |-- mwebv: float (nullable = true)\n",
    " |    |-- z_final: float (nullable = true)\n",
    " |    |-- z_final_err: float (nullable = true)\n",
    " |    |-- hostgal_zspec: float (nullable = true)\n",
    " |    |-- hostgal_zspec_err: float (nullable = true)\n",
    " |    |-- hostgal_zphot: float (nullable = true)\n",
    " |    |-- hostgal_zphot_err: float (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117fc090-4a6b-4532-b1bb-ccbad87984b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change = df.select(\n",
    "        df.diaSource.midpointTai,\n",
    "        df.diaSource.filterName,\n",
    "        df.diaSource.psFlux,\n",
    "        df.diaSource.psFluxErr,\n",
    "        df.diaObject.mwebv,\n",
    "        df.diaObject.z_final,\n",
    "        df.diaObject.z_final_err,\n",
    "        df.diaObject.hostgal_zphot,\n",
    "        df.diaObject.hostgal_zphot_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81927e54-d0e7-4120-a7ef-ebdd7b89cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = (df_change\n",
    "    .withColumnRenamed(\"diaSource.midpointTai\", \"midpointTai\")\n",
    "    .withColumnRenamed(\"diaSource.midpointTai\", \"midpointTai\")\n",
    "    .withColumnRenamed(\"diaSource.filterName\", \"filterName\")\n",
    "    .withColumnRenamed(\"diaSource.psFlux\", \"psFlux\")\n",
    "    .withColumnRenamed(\"diaSource.psFluxErr\", \"psFluxErr\")\n",
    "    .withColumnRenamed(\"diaObject.mwebv\", \"mwebv\")\n",
    "    .withColumnRenamed(\"diaObject.z_final\", \"z_final\")\n",
    "    .withColumnRenamed(\"diaObject.z_final_err\", \"z_final_err\")\n",
    "    .withColumnRenamed(\"diaObject.hostgal_zphot\", \"hostgal_zphot\")\n",
    "    .withColumnRenamed(\"diaObject.hostgal_zphot_err\", \"hostgal_zphot_err\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ad6436-34e1-471b-8992-63d49323b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_col(\n",
    "        df, colname: str, prefix: str = 'c',\n",
    "        current: str = 'candidate', history: str = 'prv_candidates'):\n",
    "    \"\"\" Add new column to the DataFrame named `prefix`+`colname`, containing\n",
    "    the concatenation of historical and current measurements.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Pyspark DataFrame containing alert data\n",
    "    colname: str\n",
    "        Name of the column to add (without the prefix)\n",
    "    prefix: str\n",
    "        Additional prefix to add to the column\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "            prefix + colname,\n",
    "            F.when(\n",
    "                df['{}.{}'.format(history, colname)].isNotNull(),\n",
    "                F.concat(\n",
    "                    df['{}.{}'.format(history, colname)],\n",
    "                    F.array(df['{}.{}'.format(current, colname)])\n",
    "                )\n",
    "            ).otherwise(F.array(df['{}.{}'.format(current, colname)]))\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb105f0-b9f8-4e2f-b7cd-35a8543ee6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from matplotlib) (1.19.5)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.37.1-py3-none-any.whl (957 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m957.2/957.2 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.37.1 kiwisolver-1.4.4 matplotlib-3.5.3 pillow-9.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff89cd2-d6cc-4d92-be51-724c33935870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328,) (292,) (328,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,) (78,) (64,)\n"
     ]
    }
   ],
   "source": [
    "what = ['midPointTai', 'psFlux', 'psFluxErr']\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for element in what:\n",
    "    df = concat_col(df, element, current='diaSource', history='prvDiaForcedSources')\n",
    "    \n",
    "#df.toPandas()['cmidPointTai'].values[2:4]):\n",
    "\n",
    "for i, mjd in enumerate(np.array(df.select('cmidPointTai').collect())[2:4]):\n",
    "    flux = np.array(df.select('cpsFlux').collect())[i][0]\n",
    "    flux_err = np.array(df.select('cpsFluxErr').collect())[i][0]\n",
    "    \n",
    "    print(np.array(flux_err).shape,  np.array(mjd[0]).shape, np.array(flux).shape)\n",
    "    #lc = np.vstack([mjd[0], flux, flux_err]).T\n",
    "    \n",
    "    \n",
    "    \n",
    "#plt.plot(lc[:, 0], lc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b9d654-5178-42bc-8f69-c03f1ed3ac07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14333/1557960260.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df_new_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e21ce5cc-2cf5-4942-bd8c-8354bffa529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+---------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|midpointTai|filterName|   psFlux|psFluxErr|      mwebv|    z_final|z_final_err|hostgal_zphot|hostgal_zphot_err|\n",
      "+-----------+----------+---------+---------+-----------+-----------+-----------+-------------+-----------------+\n",
      "| 61125.1297|         z| 32415.53|926.96313| 0.09092952|       -9.0|       -9.0|         -9.0|             -9.0|\n",
      "| 61125.1456|         z|28024.938|535.88776| 0.06947285| 0.08547318|    0.02945|   0.08547318|          0.02945|\n",
      "| 61125.1465|         z| 6534.219|1155.5437|0.046737716|       -9.0|       -9.0|         -9.0|             -9.0|\n",
      "| 61125.2011|         i|494.15225|280.01733| 0.02286238| 0.12899986|    0.03063|   0.12899986|          0.03063|\n",
      "| 61125.2301|         g|815.78754|109.06171| 0.06075122|  0.9675264|    0.05358|    0.9675264|          0.05358|\n",
      "| 61125.2393|         g|6045.9805| 506.4182| 0.10580315|       -9.0|       -9.0|         -9.0|             -9.0|\n",
      "| 61125.2591|         r|2052.0627|197.06195| 0.09976872|  0.2379598|    0.03307|    0.2379598|          0.03307|\n",
      "| 61125.2694|         r| 2768.232| 179.9869| 0.08503467|  0.9621579|    0.05218|    0.9621579|          0.05218|\n",
      "| 61125.2748|         r|1716.8215|224.08382|0.036916196|  1.5873355|    0.06997|    1.5873355|          0.06997|\n",
      "| 61125.2861|         r|3816.0425| 178.7731| 0.05542278| 0.47472093|    0.04007|   0.47472093|          0.04007|\n",
      "|  61125.292|         r|3056.6409|163.16498|0.042297278| 0.83875185|    0.04931|   0.83875185|          0.04931|\n",
      "| 61125.2938|         r|2174.4536|193.80867|0.043298025|  0.3390697|    0.03632|    0.3390697|          0.03632|\n",
      "| 61125.3149|         i|2721.5923|238.16248|0.046813384| 0.75110537|    0.04643|   0.75110537|          0.04643|\n",
      "|  61125.384|         i|3077.6108|402.98672|0.029185403|  0.3494156|    0.03589|    0.3494156|          0.03589|\n",
      "| 61125.4083|         z|5346.3687|776.04584| 0.07003998| 0.34845144|    0.03607|   0.34845144|          0.03607|\n",
      "| 61125.0342|         z|16030.298| 859.8812| 0.10614451|       -9.0|       -9.0|         -9.0|             -9.0|\n",
      "| 61125.1348|         z|62473.277|1330.0641| 0.07945534|       -9.0|       -9.0|         -9.0|             -9.0|\n",
      "| 61125.1538|         z|1790.2617|370.26254|0.060236912|  0.3875605|    0.03788|    0.3875605|          0.03788|\n",
      "| 61125.2341|         g|1643.0355|138.08337| 0.18670271|0.050310683|    0.02792|  0.050310683|          0.02792|\n",
      "| 61125.2449|         g|399.64124| 97.19688| 0.09667083|  1.2636334|    0.06114|    1.2636334|          0.06114|\n",
      "+-----------+----------+---------+---------+-----------+-----------+-----------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5cf29-803e-436e-999c-fe83fd2e301d",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95d28270-b427-4310-8a1c-1eb65f495d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "def normalize_light_curve(lc_array: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Normalize light curves.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lc_array: 1D np.array\n",
    "        Input light curve of an alert.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    result: np.array\n",
    "        normalized light curve of an alert.\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.zeros((lc_array.shape[0],))\n",
    "    result[:] = lc_array[:]\n",
    "    result[:] -= lc_array[0]\n",
    "\n",
    "    norm = (lc_array[:] - np.min(lc_array[:])) / np.ptp(lc_array[:])\n",
    "    result[:] = norm\n",
    "\n",
    "    return result\n",
    "\n",
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def map_filter_name(filter_name: pd.Series) -> pd.Series:\n",
    "    filter_dict = {'u':1, 'g':2, 'r':3, 'i':4, 'z':5, 'Y':6}\n",
    "    if len(filter_name) < 10:\n",
    "    \n",
    "        filter_name = filter_name.copy()\n",
    "        filter_name.map(filter_dict)\n",
    "    \n",
    "    else:\n",
    "        nfilter_name = filter_name\n",
    "        \n",
    "    return pd.Series(filter_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "849aa13f-2bab-469f-84a4-9019e6b33c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType(), PandasUDFType.SCALAR)\n",
    "def predict_nn(\n",
    "        midpointTai: pd.Series, psFlux: pd.Series, psFluxErr: pd.Series,\n",
    "        filterName: pd.Series,\n",
    "        mwebv: pd.Series, z_final: pd.Series,\n",
    "        z_final_err: pd.Series, hostgal_zphot: pd.Series,\n",
    "        hostgal_zphot_err: pd.Series\n",
    "    ) -> pd.Series:\n",
    "    \n",
    "    \"\"\"\n",
    "    Return predctions from a model given inputs as pd.Series\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    midpointTai: spark DataFrame Column\n",
    "        SNID JD Time (float)\n",
    "    psFlux: spark DataFrame Column\n",
    "        flux from LSST (float)\n",
    "    psFluxErr: spark DataFrame Column\n",
    "        flux error from LSST (float)\n",
    "    filterName:\n",
    "        (string)\n",
    "    mwebv:\n",
    "        (float)\n",
    "    z_final: spark DataFrame Column\n",
    "        redshift of a given event (float)\n",
    "    z_final_err: spark DataFrame Column\n",
    "        redshift error of a given event (float)       \n",
    "    hostgal_zphot: spark DataFrame Column\n",
    "        photometric redshift of host galaxy (float)\n",
    "    hostgal_zphot_err: spark DataFrame Column\n",
    "        error in photometric redshift of host galaxy (float)\n",
    "    Returns:\n",
    "    --------\n",
    "    preds: pd.Series\n",
    "        predictions of a broad class in an pd.Series format (pd.Series[float])\n",
    "    \"\"\"\n",
    "    \n",
    "    bands = []\n",
    "    lcs = []\n",
    "    meta = []\n",
    "\n",
    "    filterName = apply_filter_name(filterName)\n",
    "    #new_midpointTai = normalize_light_curve(midpointTai)\n",
    "    \n",
    "    \n",
    "    return pd.Series(filterName)\n",
    "    #return pd.Series(midpoinTai)\n",
    "\n",
    "    for i, mjds in enumerate(midpointTai):\n",
    "        print(i, mjds)\n",
    "        bands.append(np.array(\n",
    "             [filter_dict[f] for f in filterName[i]]\n",
    "         ).astype(np.int16))\n",
    "        \n",
    "    return pd.Series(bands)\n",
    "#         lc = np.concatenate(\n",
    "            [mjds[:,None], psFlux[i][:,None], psFluxErr[:,None]], axis=-1\n",
    "            )\n",
    "        lcs.append(normalize_lc(lc).astype(np.float32))\n",
    "        meta.append(\n",
    "            np.concatenate(\n",
    "                [mwebv[i], z_final[i], z_final_err[i], hostgal_zphot[i], hostgal_zphot_err[i]]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    X = {\n",
    "        'meta': np.array(meta),\n",
    "        'band': tf.RaggedTensor.from_row_lengths(\n",
    "            values=tf.concat(bands, axis=0),\n",
    "            row_lengths=[a.shape[0] for a in bands]\n",
    "        ),\n",
    "\n",
    "        'lc': tf.RaggedTensor.from_row_lengths(\n",
    "            values=tf.concat(lcs, axis=0),\n",
    "            row_lengths=[a.shape[0] for a in lcs]\n",
    "        )\n",
    "    }\n",
    "    for i, x in enumerate(X['meta'][:,3]):\n",
    "        if x < 0:\n",
    "            X['meta'][i,1:] = -1\n",
    "        else:\n",
    "            X['meta'][i,1:] = x\n",
    "\n",
    "    model='../models/model_test_meta_ragged_1det_after/'\n",
    "    NN = tf.keras.models.load_model(model)\n",
    "    preds = NN.predict(X)\n",
    "    \n",
    "    return pd.Series(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "378a25fd-a67d-4a9a-8002-43f9dfdab32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 16:24:30 ERROR Executor: Exception in task 2.0 in stage 56.0 (TID 139)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n",
      "    for series in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n",
      "    return lambda *a: (verify_result_length(*a), arrow_return_type)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n",
      "    result = f(*a)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n",
      "  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\n",
      "NameError: name 'new_filter_name' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/09/06 16:24:30 WARN TaskSetManager: Lost task 2.0 in stage 56.0 (TID 139, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n",
      "    for series in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n",
      "    return lambda *a: (verify_result_length(*a), arrow_return_type)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n",
      "    result = f(*a)\n",
      "  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n",
      "  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\n",
      "NameError: name 'new_filter_name' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/09/06 16:24:30 ERROR TaskSetManager: Task 2 in stage 56.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1413.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 56.0 failed 1 times, most recent failure: Lost task 2.0 in stage 56.0 (TID 139, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\nNameError: name 'new_filter_name' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\nNameError: name 'new_filter_name' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41461/1915201872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                        \u001b[0;34m'z_final_err'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                        \u001b[0;34m'hostgal_zphot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                        \u001b[0;34m'hostgal_zphot_err'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                    )\n\u001b[1;32m     16\u001b[0m                   ).show()\n",
      "\u001b[0;32m/home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/anaconda3/envs/lsst_dev37/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1413.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 56.0 failed 1 times, most recent failure: Lost task 2.0 in stage 56.0 (TID 139, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\nNameError: name 'new_filter_name' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/home/andsantos/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41461/3994271690.py\", line 49, in predict_nn\n  File \"/tmp/ipykernel_41461/2965501951.py\", line 37, in apply_filter_name\nNameError: name 'new_filter_name' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_data.withColumn('mapFilterName',\n",
    "                   predict_nn(\n",
    "                       'midpointTai',\n",
    "                       'filterName',\n",
    "                       'psFlux',\n",
    "                       'psFluxErr',\n",
    "                       'mwebv',\n",
    "                       'z_final',\n",
    "                       'z_final_err',\n",
    "                       'hostgal_zphot',\n",
    "                       'hostgal_zphot_err',\n",
    "                   )\n",
    "                  ).show()\n",
    "\n",
    "#df_data.withColumn('normalized', df_data['filterName']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d49bc-c47e-40db-86a6-2c5cf2b8c5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d3d2bdf96b9702ae5c09f5b87104f3581aa4e7c5220b24b8cd1d87e3f57d61e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
