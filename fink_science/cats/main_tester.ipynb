{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc43cf7-bbf3-46ac-a2ed-f569131b6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746c9a3-a3a5-4dfe-b5e9-2918d59b2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .getOrCreate()\n",
    "        )\n",
    "DATA_DIR='/home/data/phd/research_projects/fink_science/fink_science/cbpf_classifier/data/alerts/'\n",
    "df = (spark\n",
    "         .read\n",
    "         .format('parquet')\n",
    "         .load(DATA_DIR+'small_sample.parquet')\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80f55a-66c5-4797-b8d1-4260b9283293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fink_utils.spark.utils import concat_col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "cols = ['midPointTai', 'psFlux', 'psFluxErr', 'filterName']\n",
    "\n",
    "for col_ in cols:\n",
    "    df = concat_col(df, col_, prefix='c', current='diaSource', history='prvDiaSources')\n",
    "    \n",
    "colsc = ['c'+i for i in cols]\n",
    "\n",
    "colnames = [\n",
    "    'cmidPointTai', 'cpsFlux', 'cpsFluxErr', 'cfiltername', \n",
    "    'diaObject.mwebv', 'diaObject.z_final', 'diaObject.z_final_err',\n",
    "    'diaObject.hostgal_zphot', 'diaObject.hostgal_zphot_err', \n",
    "    lit('/home/data/phd/research_projects/fink_science/fink_science/cbpf_classifier/models/model_test_meta_ragged_1det_after')\n",
    "]\n",
    "\n",
    "#df_sub = df.select(colnames)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow_addons import optimizers\n",
    "from utilities import normalize_lc\n",
    "\n",
    "tf.optimizers.RectifiedAdam = optimizers.RectifiedAdam\n",
    "\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def predict_nn(\n",
    "        midpointTai: pd.Series, psFlux: pd.Series, psFluxErr: pd.Series,\n",
    "        filterName: pd.Series, mwebv: pd.Series, z_final: pd.Series,\n",
    "        z_final_err: pd.Series, hostgal_zphot: pd.Series,\n",
    "        hostgal_zphot_err: pd.Series,\n",
    "        model\n",
    "        ) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return predctions from a model given inputs as pd.Series\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    midpointTai: spark DataFrame Column\n",
    "        SNID JD Time (float)\n",
    "    psFlux: spark DataFrame Column\n",
    "        flux from LSST (float)\n",
    "    psFluxErr: spark DataFrame Column\n",
    "        flux error from LSST (float)\n",
    "    filterName:\n",
    "        (string)\n",
    "    mwebv:\n",
    "        (float)\n",
    "    z_final: spark DataFrame Column\n",
    "        redshift of a given event (float)\n",
    "    z_final_err: spark DataFrame Column\n",
    "        redshift error of a given event (float)       \n",
    "    hostgal_zphot: spark DataFrame Column\n",
    "        photometric redshift of host galaxy (float)\n",
    "    hostgal_zphot_err: spark DataFrame Column\n",
    "        error in photometric redshift of host galaxy (float)\n",
    "    model: spark DataFrame Column\n",
    "        path to pre-trained Hierarchical Classifier model. (string)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preds: pd.Series\n",
    "        predictions of a broad class in an pd.Series format (pd.Series[float])\n",
    "    \"\"\"\n",
    "\n",
    "    filter_dict = {'u':1, 'g':2, 'r':3, 'i':4, 'z':5, 'Y':6}\n",
    "    \n",
    "    class_dict = {\n",
    "        0: 111,\n",
    "        1: 112,\n",
    "        2: 113,\n",
    "        3: 114,\n",
    "        4: 115,\n",
    "        5: 121,\n",
    "        6: 122,\n",
    "        7: 123,\n",
    "        8: 124,\n",
    "        9: 131,\n",
    "        10: 132,\n",
    "        11: 133,\n",
    "        12: 134,\n",
    "        13: 135,\n",
    "        14: 211,\n",
    "        15: 212,\n",
    "        16: 213,\n",
    "        17: 214,\n",
    "        18: 221\n",
    "    }\n",
    "        \n",
    "    bands = []\n",
    "    lcs = []\n",
    "    meta = []\n",
    "\n",
    "    for i, mjds in enumerate(midpointTai):\n",
    "        \n",
    "        if len(mjds) > 0:\n",
    "            bands.append(np.array(\n",
    "                [filter_dict[f] for f in filterName.values[i]]\n",
    "            ).astype(np.int16))        \n",
    "            lc = np.concatenate(\n",
    "                [mjds[:,None], psFlux.values[i][:,None], psFluxErr.values[i][:,None]], axis=-1\n",
    "                )\n",
    "            \n",
    "            \n",
    "            \n",
    "            if not np.isnan(mwebv.values[i]):\n",
    "                \n",
    "                lcs.append(normalize_lc(lc).astype(np.float32))\n",
    "                \n",
    "                # print([\n",
    "                #             mwebv.values[i], z_final.values[i],\n",
    "                #             z_final_err.values[i], hostgal_zphot.values[i],\n",
    "                #             hostgal_zphot_err.values[i]\n",
    "                #         ])\n",
    "                \n",
    "                meta.append([\n",
    "                            mwebv.values[i], z_final.values[i],\n",
    "                            z_final_err.values[i], hostgal_zphot.values[i],\n",
    "                            hostgal_zphot_err.values[i]\n",
    "                        ])\n",
    "           \n",
    "    \n",
    "    X = {\n",
    "        'meta': np.array(meta),\n",
    "        'band': tf.RaggedTensor.from_row_lengths(\n",
    "            values=tf.concat(bands, axis=0),\n",
    "            row_lengths=[a.shape[0] for a in bands]\n",
    "        ),\n",
    "\n",
    "        'lc': tf.RaggedTensor.from_row_lengths(\n",
    "            values=tf.concat(lcs, axis=0),\n",
    "            row_lengths=[a.shape[0] for a in lcs]\n",
    "        )\n",
    "    }\n",
    "    for i, x in enumerate(X['meta'][:,3]):\n",
    "        if x < 0:\n",
    "            X['meta'][i,1:] = -1\n",
    "        else:\n",
    "            X['meta'][i,1:] = x\n",
    "\n",
    "    NN = tf.keras.models.load_model(model.values[0], custom_objects={'RectifiedAdam': optimizers.RectifiedAdam})\n",
    "    preds = NN.predict(X)\n",
    "    \n",
    "    \n",
    "    return pd.Series([class_dict[p.argmax()] for p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b24260-7800-4353-bfea-492fc36f4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7be80a-d2e2-4b3e-9a36-bd9930467c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('pBroadClass', predict_nn(*colnames))\n",
    "df.select('pBroadClass').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328c4b3-24f5-49b3-adda-6b7bd8d16593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59436559-71b3-44d9-a509-9d1cf7f76275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05049daa-92ff-4537-8586-d5c534240c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(df['diaSource']['diaSourceId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dece90-ddc2-429c-b3db-74da80e46fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select((df\n",
    "#            .diaSource\n",
    "#            .diaSourceId\n",
    "#           )\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020f309-41e8-4b8b-9754-aeb77670c8a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### pypsark DataFrame format (only showed features used by classifier)\n",
    "\n",
    "```\n",
    "root\n",
    " |-- alertId: long (nullable = true)\n",
    " |-- diaSource: struct (nullable = true)\n",
    " |    |-- midPointTai: double (nullable = true)\n",
    " |    |-- filterName: string (nullable = true)\n",
    " |    |-- psFlux: float (nullable = true)\n",
    " |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaSources: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |    |    |-- psFlux: float (nullable = true)\n",
    " |    |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaForcedSources: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |    |    |-- psFlux: float (nullable = true)\n",
    " |    |    |-- psFluxErr: float (nullable = true)\n",
    " |-- prvDiaNondetectionLimits: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- midPointTai: double (nullable = true)\n",
    " |    |    |-- filterName: string (nullable = true)\n",
    " |-- diaObject: struct (nullable = true)\n",
    " |    |-- mwebv: float (nullable = true)\n",
    " |    |-- z_final: float (nullable = true)\n",
    " |    |-- z_final_err: float (nullable = true)\n",
    " |    |-- hostgal_zspec: float (nullable = true)\n",
    " |    |-- hostgal_zspec_err: float (nullable = true)\n",
    " |    |-- hostgal_zphot: float (nullable = true)\n",
    " |    |-- hostgal_zphot_err: float (nullable = true)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d3d2bdf96b9702ae5c09f5b87104f3581aa4e7c5220b24b8cd1d87e3f57d61e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
